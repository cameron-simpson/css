vt.dir: FileSystem state: a mapping of UUID->Dirent for the indirect dirents - where to keep this context at runtime for indirection?
vt.dir: new IndirectDirent pointing to another Dirent by UUID
vt.fs: split off the FileHandle index into a FileHandles class to look after the fhndx allocation
vt.stream: give the _connect method some hysteresis
DataDir: scan progress: poll all files, keep .total = total size and .position = scanned size, update on each monitor pass and as scanned - attach Progress to DataFiles?
general purpose file tree monitor yielding (rpath, old_state, new_state), start with os.walk, switch to inotify andf fsevents later if available
DataDir: compute current on open as the newest data file less than maxsize-maxblocksize, drop from state CSV - what to do about shared append to the file?
vt.fs: does Inodes._hardlinked need to exist?
vt.fs.FileSystem: make the modes into a set, simplify the initialiser
Dir.D_*_T values to be IntEnums
ShadowDir(dirpath, Dir, archive): make dirpath to be like Dir, apply updates from archive, push local changes to Dir and archive
get rid of uses of totext/fromtext and dirent_fromtext etc
sockets: clean up shutdown process: handlers to honour the shared runstate, not abort so much, kill shutdown_now
Config: parser which returns pure dict of dicts, Config constructed from wired-in defaults plus config file
socket Stores: just use path instead of socket_path?
streamstore: Client handler: make current Store local to it for independence
move CancellationError into cs.resources, associate with RunState
readahead for CornuCopyBuffer?
socket store: export = name[:storespec],... with default being "default:{defaults.S}"
socket store: dflt socket_path = {basedir}/{clausename}.sock
make datadirs sharable by multiple vt instances: shared state csv
cs.vt.socket_tests: UNIX domain socket tests
ShadowStore(shadowdir,backend): monitor real shadowdir, propagate changes to backend - inverse of PlatonicStore
vt setup/tutorial
vt-blockmap.5 man entry
vt-datadir.5 man entry
vtd.5 man entry
vtrc.5 man entry
vt.1 man entry
blockmap updates - use lower priority than regular I/O
platonic updates - use lower priority than regular I/O
Store.archive plumbing
ReadMixin: some kind of optional readahead if bfr reused and emptier than some threshold at return from read
Store.archive([name]) ==> an Archive instance for state updates for Dirs
[clause].archives_from = glob:storespec ... to plumb archives to backend Stores, or simply other Stores, eg "*:[metadata]"
default virtual .vtrc contents, option like ssh -G to dump config file maybe "vt config" or a -n option
PlatonicStore:
  keep mapping of paths => (mtime, size) to monitor changes
  update should rescan whole file, toss old hashcode mapping for that file
Dirent:
  metadata st_ctime - useful for reconciliation/update
  S_IFWHT whiteout entries
FS:
  file system layer on top of Dir etc
  split out from vtfuse
  inodes derived from uuids, but only on request or for hardlinks
  vtfuse: no .. in top dir? check mount point .. impl
config:
  Config object with singleton mapping to clauses
  clauses to support direct [key]=value for access and update
.vtd export format:
  block 0 is indirect ref to top dirent contents

index of hashcode to remote store refs, to support multistore cooperation and proxying
stream protocol: error flag, json flag (implies JSON additional payload)
stream protocol: new channel ==> recursive substream
flat file cache - keep multiple caches, set of hashes per file,
  upper limit, drop oldest file once set empty and >8 files
mount -o append: accept O_TRUNC for empty/missing files
audit fs: append only, no deletes, maybe no renames
mount -a: live mode, tracing new .vt entries
mount -e command...
test vtfuse MT functionality: 2 threads, then 16
xfstests
daemon: listen on UNIX domain socket
  test access to socket via ssh forwarding
ticker to regularly sync the fs
import: import_file for existing files: efficient content comparison etc
    prepare a CornuCopyBuffer fed from the existing vt blocks as the leading comparison process blockifier, use .take to 
    compare a pair of CornuCopyBuffers using one as the reference blockifer source
vt level file ops using setxattr? nasty but the only way? clone, patch, snip, assemble-archive
    control: x-vt-control=op (set-content, splice, crop, compose, ...)
    set: x-vt-clone=new-rpath
    set: Dir:x-vt-archive-as:type[=name], archive available as name (or computed default)
        tar, iso, zip(?), udf(?)
        vtd: complete content: top block first (always an IndirectBlock), then metadata/file tree, then data blocks
meta: user.mime_type
"raw datafile store": point at a .vtd datafile and kick off a scanner,
  fetch will block until scanned if missing hash
  for use with "vt ar"
  "file:datafile.vtd" Store scheme
recognise .foo.ext.xxxxxx rsync temp files, infer scanner from .ext embedded extension
scanner: if scanner is None, probe first 1kb of file content to infer type, should help with rsync etc
"live" xattr: x-vt-blockref => block reference as text
    causes a sync, doesn't return until ready; should be ok multithreaded
vtfuse close file should not block, but update mtime and queue sync with callback to update Block on Dirent - wait for all same on fs sync/umount
  start syncing appended data immediately?
    => better file-close behaviour
SIGINT clean shutdown of mount
make a single NUL-filled bytes object, get literals to return memoryviews of it for various sized NUL blocks
    NULBlock trite factory for RLEBlock(b'\0', length)
salt entries for Dirs,
stats in new vs existing blocks when adding blocks
scanners:
  mbox look for nn and rnrn
  scanner for .vtd files, supporting efficient storage of stores
  scanner for Dirents, for blockifying Dirs
is ctypes.addressof any use for i/o?
blockify
  sniff files to infer high level syntax
  histograms on blockify block sizes
  scanner for .gz possible?
  offset/chunk queue uses mutexes to store at most 1 offset or
    chunk - in fact maybe not a queue at all - general queuelike
    thingy with - maybe a 2 element heap with 1 element queues
    feeding it or a pair with channels feeding it
    problem: offset many chunks in the future?
  blockify Dir encodings: top_block_of(Dirent-chunks-of-entries)
vtfuse
  store content-type in xattrs
  FileHandle use raw file instead of stdio
  support lseek SEEK_HOLE and SEEK_DATA T v
File.close:
  get a preferred scanner in from outside
  pass the scanner to blockify
  keep a partial_block bytes for use at the front of the filedata
  modify the backing data part to examine the last B
    update partial_block with it instead of yielding it if it is a partial
decode-Dir: use copy buffer and leaf blocks
decode binary stream: use copy buffer
control module
  vtftp to be an API to it
BackedFile
  set change-on-close flag on write/truncate etc
  raw file with rfd and wfd for front file
  support _only_ pread and pwrite methods for I/O
move to SHA-3-256, check hash size etc
  v/transition.py: reindex Stores, transcribe one Dirent to its equivalent
    for k in S1: S2.add[S1[k]
    for E in S1.walk(D):
      E2 = copy of E using S2
      flush E2
    think about ways to work against 2 Stores, current per thread
      default store doesn't work here; pipe info to another thread?
support ranges on GET requests
URIs:
  x-vt:[//host[:port]]/textblockref-of-Dir/path...
vt publish pathname
  construct and Store Dir:{basename(pathname)=>tree} and recite dirent as x-vt:/textblockref/basename/
ftp(Dir)
  Dir can be the basis for a mount, or from a blockref etc
  CD path                     Change working path
  INSPECT name                Report metadata
  PEER other-store-name
  GET name [local-fs-name]    Export tree/file
  PUT local-fs-name [name]    Import tree/file
  BIND name textdirent        Attach existing tree/file
  PULL name   # needs peer    Fill in missing Blocks for name from peer
  PUSH name   # needs peer    Export Blocks for name to peer
  join/merge live mount points
  QUIT => sync and recite top Dirref?
vtfuse:
  OSX Finder name of mountpoint
  umount: drop inode_data if empty (no hard links)?
          predrop of inodes with < 2 links?
  open of symlink
  do not sync unlinked files
  include/exclude rules, like rsync?
    * use a context when computing Dir.block etc, thus usable outside vtfuse
    do not sync (or Store?) excluded items
    need to promote unbacked files to backed at fs sync time based on name?
  include/exclude mount options
    include/exclude general syntax?
  support multiple mount points?
    off a single "live" antecedant Dir?
  control:
    control channel/cmd line?
    link pathname to blockspec
    merge trees
Dir:
  rsync -a: setgid bit not preserved? possibly fuse nosetuid mount setting
  vtftp command which accesses a Dir
    hook to vtfuse to run vtftp against live Dir
datadir
  ticker to sync gdbm index? or just on _indexQ empty?
  report degree of gdbm batching
  maxsize setting, to be used for caches
  file monitoring: tail() feeding to data block scanner;
    data block scanner to use copy buffer
ProgressStore
  proxy for a single subStore with various Progress attributes .progress_*
  convenient status line:
    {progress_add_bytes|human}:{progress_add_count} {progress_get_bytes|human} {progress_outstanding}
S3Store
  stores Blocks directly as texthashcode.{hashname}
HTTPStore
  /texthashcode.{hashname}
HTTPDaemon
  /h/texthashcode.sha1 block (redirect to other http? eg an S3 backed one)
  /i/texthashcode.sha1 indirect block contents
  /d/textblockref/... Dir
    /d/textblockref/path/to/file content (internally retrieves content, presents with Content-Type)
CloundFront ==> HTTPDaemon (possible to map /h/ directly to separate S3?)
SyncProcess: context manager object performing some long operation
  .progress_{total|outstanding|done}
  SyncBlock(Block, local, remote): fill missing Blocks: for a Block, itself; for an IndirectBlock, also its contents
  SyncDir(Dirent): pull in contents of dir, optionally including file contents
blockify
  sniff files to infer high level syntax
  parsers:
    blockify should cope with parsers which exception
    should blockify dup the source chunks and verify that the
      chunks from the parser match the chunks from the source?
    would that imply the parsers need only consume the chunks and
      emit offsets, and not need to emit chunks?
      that would avoid needing verification, and possibly avoid
        some chunk reblocking a parser might do, and also allow
        us to cope with parsers that exception out
      * still need to accomodate data generation with offsets, eg big
        Dirents being emitted with Dirent boundaries
control module
  vtftp to be an API to it
BackedFile to set change-on-close flag on write/truncate etc
move to SHA-3-256, check hash size etc
  v/transition.py: reindex Stores, transcribe one Dirent to its equivalent
    for k in S1: S2.add[S1[k]
    for E in S1.walk(D):
      E2 = copy of E using S2
      flush E2
    think about ways to work against 2 Stores, current per thread
      default store doesn't work here; pipe info to another thread?
support ranges on GET requests
URIs:
  x-vt:[//host[:port]]/textblockref-of-Dir/path...
vt publish pathname
  construct and Store Dir:{basename(pathname)=>tree} and recite dirent as x-vt:/textblockref/basename/
ftp(Dir)
  Dir can be the basis for a mount, or from a blockref etc
  CD path                     Change working path
  INSPECT name                Report metadata
  PEER other-store-name
  GET name [local-fs-name]    Export tree/file
  PUT local-fs-name [name]    Import tree/file
  BIND name textdirent        Attach existing tree/file
  PULL name   # needs peer    Fill in missing Blocks for name from peer
  PUSH name   # needs peer    Export Blocks for name to peer
  join/merge live mount points
  QUIT => sync and recite top Dirref?
file
  start syncing appended data immediately?
    => better file-close behaviour
vtfuse:
  OSX Finder name of mountpoint
  umount: drop inode_data if empty (no hard links)?
          predrop of inodes with < 2 links?
  open of symlink
  do not sync unlinked files
  include/exclude rules, like rsync?
    * use a context when computing Dir.block etc, thus usable outside vtfuse
    do not sync (or Store?) excluded items
    need to promote unbacked files to backed at fs sync time based on name?
  include/exclude mount options
    include/exclude general syntax?
  support multiple mount points?
    off a single "live" antecedant Dir?
  control:
    control channel/cmd line?
    link pathname to blockspec
    merge trees
Dir:
  rsync -a: setgid bit not preserved? possibly fuse nosetuid mount setting
  vtftp command which accesses a Dir
    hook to vtfuse to run vtftp against live Dir
datadir
  ticker to sync gdbm index? or just on _indexQ empty?
  report degree of gdbm batching
  maxsize setting, to be used for caches
ProgressStore
  proxy for a single subStore with various Progress attributes .progress_*
  convenient status line:
    {progress_add_bytes|human}:{progress_add_count} {progress_get_bytes|human} {progress_outstanding}
S3Store
  stores Blocks directly as texthashcode.sha1
HTTPStore
  /texthashcode.sha1
HTTPDaemon
  /h/texthashcode.sha1 block (redirect to other http? eg an S3 backed one)
  /i/texthashcode.sha1 indirect block contents
  /d/textblockref/... Dir
    /d/textblockref/path/to/file content (internally retrieves content, presents with Content-Type)
CloudFront ==> HTTPDaemon (possible to map /h/ directly to separate S3?)
SyncProcess: context manager object performing some long operation
  .progress_{total|outstanding|done}
  SyncBlock(Block, local, remote): fill missing Blocks: for a Block, itself; for an IndirectBlock, also its contents
  SyncDir(Dirent): pull in contents of dir, optionally including file contents
how is video stored? decoder for common formats
