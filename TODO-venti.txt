SIGINT clean shutdown of mount
make a single NUL-filled bytes object, get literals to return memoryviews of it for various sized NUL blocks
recognise .foo.ext.xxxxxx rsync temp files, infer scanner from .ext embedded extension
cs.buffer.CornuCopyBuffer: automatically upgrade iterables to iterators?
salt entries for Dirs,
text serialisation: all human readable, regular syntax
"live" xattr: x-vt-blockref => block reference as text
vt ar yields .vtd file whose first block is a LiteralDirent and
  all dependent Blocks are in the vtd file
"raw datafile store": point at a datafile and kick off a scanner,
  fetch will block until scanned if missing hash
  for use with "vt ar"
  "file:datafile.vtd" Store scheme
daemon: listen on UNIX domain socket
  test access to socket via ssh forwarding
stats in new vs existing blocks when adding blocks
flat file cache - keep multiple caches, set of hashes per file,
  pper limit, drop oldest file once set empty and >8 files
datafile: use os.open(O_APPEND) and raw fds, no buffering, no
  seek, no flush, separate fd for read
parsers:
  mbox look for nn and rnrn
  parser for .vtd files, supporting efficient storage of stores
  parser for Dirents, for blockifying Dirs
is ctypes.addressof any use for i/o?
blockify
  sniff files to infer high level syntax
  histograms on blockify block sizes
  parser for .gz possible?
  offset/chunk queue uses mutexes to store at most 1 offset or
    chunk - in fact maybe not a queue at all - general queuelike
    thingy with - maybe a 2 element heap with 1 element queues
    feeding it or a pair with channels feeding it
    problem: offset many chunks in the future?
  blockify Dir encodings: top_block_of(Dirent-chunks-of-entries)
stream protocol: error flag, json flag (implies JSON additional payload)
vtfuse
  store content-type in xattrs
  FileHandle use raw file instead of stdio
  support lseek SEEK_HOLE and SEEK_DATA T v
File.close:
  get a preferred parser in from outside
  pass the parser to blockify
  keep a partial_block bytes for use at the front of the filedata
  modify the backing data part to examine the last B
    update partial_block with it instead of yielding it if it is a partial
decode-Dir: use copy buffer and leaf blocks
decode binary stream: use copy buffer
control module
  vtftp to be an API to it
BackedFile
  set change-on-close flag on write/truncate etc
  raw file with rfd and wfd for front file
  support _only_ pread and pwrite methods for I/O
meta: user.mime_type
move to SHA-3-256, check hash size etc
  v/transition.py: reindex Stores, transcribe one Dirent to its equivalent
    for k in S1: S2.add[S1[k]
    for E in S1.walk(D):
      E2 = copy of E using S2
      flush E2
    think about ways to work against 2 Stores, current per thread
      default store doesn't work here; pipe info to another thread?
rename cs.venti => cs.vt
support ranges on GET requests
URIs:
  x-vt:[//host[:port]]/textblockref-of-Dir/path...
vt publish pathname
  construct and Store Dir:{basename(pathname)=>tree} and recite dirent as x-vt:/textblockref/basename/
ftp(Dir)
  Dir can be the basis for a mount, or from a blockref etc
  CD path                     Change working path
  INSPECT name                Report metadata
  PEER other-store-name
  GET name [local-fs-name]    Export tree/file
  PUT local-fs-name [name]    Import tree/file
  BIND name textdirent        Attach existing tree/file
  PULL name   # needs peer    Fill in missing Blocks for name from peer
  PUSH name   # needs peer    Export Blocks for name to peer
  join/merge live mount points
  QUIT => sync and recite top Dirref?
file
  start syncing appended data immediately?
    => better file-close behaviour
vtfuse:
  OSX Finder name of mountpoint
  umount: drop inode_data if empty (no hard links)?
          predrop of inodes with < 2 links?
  open of symlink
  do not sync unlinked files
  include/exclude rules, like rsync?
    * use a context when computing Dir.block etc, thus usable outside vtfuse
    do not sync (or Store?) excluded items
    need to promote unbacked files to backed at fs sync time based on name?
  include/exclude mount options
    include/exclude general syntax?
  support multiple mount points?
    off a single "live" antecedant Dir?
  control:
    control channel/cmd line?
    link pathname to blockspec
    merge trees
Dir:
  rsync -a: setgid bit not preserved? possibly fuse nosetuid mount setting
  vtftp command which accesses a Dir
    hook to vtfuse to run vtftp against live Dir
datadir
  ticker to sync gdbm index? or just on _indexQ empty?
  report degree of gdbm batching
  maxsize setting, to be used for caches
  file monitoring: tail() feeding to data block parser;
    data block parser to use copy buffer
ProgressStore
  proxy for a single subStore with various Progress attributes .progress_*
  convenient status line:
    {progress_add_bytes|human}:{progress_add_count} {progress_get_bytes|human} {progress_outstanding}
S3Store
  stores Blocks directly as texthashcode.{hashname}
HTTPStore
  /texthashcode.{hashname}
HTTPDaemon
  /h/texthashcode.sha1 block (redirect to other http? eg an S3 backed one)
  /i/texthashcode.sha1 indirect block contents
  /d/textblockref/... Dir
    /d/textblockref/path/to/file content (internally retrieves content, presents with Content-Type)
CloundFront ==> HTTPDaemon (possible to map /h/ directly to separate S3?)
SyncProcess: context manager object performing some long operation
  .progress_{total|outstanding|done}
  SyncBlock(Block, local, remote): fill missing Blocks: for a Block, itself; for an IndirectBlock, also its contents
  SyncDir(Dirent): pull in contents of dir, optionally including file contents
