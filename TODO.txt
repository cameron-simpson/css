vt ar yields .vtd file whose first block is a LiteralDirent and all dependent Blocks are in the vtd file
mbox look for nn and rnrn
meta: user.mime_type
is ctypes.addressof any use for i/o?
ssh-opts: cononical capitalisation of option names
cs.buffer
  unit tests
cs.fileutils:
  subfile: a file-like object proxying a byte range of another file
  tail() with blocking and nonblocking mode; follow mode? emits chunks
mp4/iso14496
  STTSBox (_GenericSampleBox): shorter rendition of .samples
cs.logutils: PfxThread to clone current prefix to new Thread, use in venti.blockify
venti:
  daemon: listen on UNIX domain socket
    test access to socket via ssh forwarding
  FIXED? BUG: new files forget their correct size on umount
  stats in new vs existing blocks when adding blocks
  flat file cache - keep multiple caches, set of hashes per file, pper limit, drop oldest file once set empty
  datafile: use os.open(O_APPEND) and raw fds, no buffering, no seek, no flush, separate fd for read
  blockify
    sniff files to infer high level syntax
    histograms on blockify block sizes
    parser for .gz possible?
  stream protocol: error flag, json flag (implies JSON additional payload)
  blockify Dir encodings: top_block_of(Dirent-chunks-of-entries)
  vtfuse
    store content-type in xattrs
    FileHandle use raw file instead of stdio
  File.close:
    get a preferred parser in from outside
    pass the parser to blockify
    keep a partial_block bytes for use at the front of the filedata
    modify the backing data part to examine the last B
      update partial_block with it instead of yielding it if it is a partial
  decode-Dir: use copy buffer and leaf blocks
  decode binary stream: use copy buffer
  control module
    vtftp to be an API to it
  BackedFile to set change-on-close flag on write/truncate etc
  move to SHA-3-256, check hash size etc
    v/transition.py: reindex Stores, transcribe one Dirent to its equivalent
      for k in S1: S2.add[S1[k]
      for E in S1.walk(D):
        E2 = copy of E using S2
        flush E2
      think about ways to work against 2 Stores, current per thread
        default store doesn't work here; pipe info to another thread?
  rename cs.venti => cs.vt
  support ranges on GET requests
  URIs:
    x-vt:[//host[:port]]/textblockref-of-Dir/path...
  vt publish pathname
    construct and Store Dir:{basename(pathname)=>tree} and recite dirent as x-vt:/textblockref/basename/
  ftp(Dir)
    Dir can be the basis for a mount, or from a blockref etc
    CD path                     Change working path
    INSPECT name                Report metadata
    PEER other-store-name
    GET name [local-fs-name]    Export tree/file
    PUT local-fs-name [name]    Import tree/file
    BIND name textdirent        Attach existing tree/file
    PULL name   # needs peer    Fill in missing Blocks for name from peer
    PUSH name   # needs peer    Export Blocks for name to peer
    join/merge live mount points
    QUIT => sync and recite top Dirref?
  file
    start syncing appended data immediately?
      => better file-close behaviour
  vtfuse:
    OSX Finder name of mountpoint
    umount: drop inode_data if empty (no hard links)?
            predrop of inodes with < 2 links?
    open of symlink
    do not sync unlinked files
    include/exclude rules, like rsync?
      * use a context when computing Dir.block etc, thus usable outside vtfuse
      do not sync (or Store?) excluded items
      need to promote unbacked files to backed at fs sync time based on name?
    include/exclude mount options
      include/exclude general syntax?
    support multiple mount points?
      off a single "live" antecedant Dir?
    control:
      control channel/cmd line?
      link pathname to blockspec
      merge trees
  Dir:
    rsync -a: setgid bit not preserved? possibly fuse nosetuid mount setting
    vtftp command which accesses a Dir
      hook to vtfuse to run vtftp against live Dir
  datadir
    ticker to sync gdbm index? or just on _indexQ empty?
    report degree of gdbm batching
    maxsize setting, to be used for caches
    file monitoring: tail() feeding to data block parser;
      data block parser to use copy buffer
  ProgressStore
    proxy for a single subStore with various Progress attributes .progress_*
    convenient status line:
      {progress_add_bytes|human}:{progress_add_count} {progress_get_bytes|human} {progress_outstanding}
  S3Store
    stores Blocks directly as texthashcode.{hashname}
  HTTPStore
    /texthashcode.{hashname}
  HTTPDaemon
    /h/texthashcode.sha1 block (redirect to other http? eg an S3 backed one)
    /i/texthashcode.sha1 indirect block contents
    /d/textblockref/... Dir
      /d/textblockref/path/to/file content (internally retrieves content, presents with Content-Type)
  CloundFront ==> HTTPDaemon (possible to map /h/ directly to separate S3?)
  SyncProcess: context manager object performing some long operation
    .progress_{total|outstanding|done}
    SyncBlock(Block, local, remote): fill missing Blocks: for a Block, itself; for an IndirectBlock, also its contents
    SyncDir(Dirent): pull in contents of dir, optionally including file contents
