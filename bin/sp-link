#!/usr/bin/env python3
#
# Utility script for dealing with SP-Link data.
# - Cameron Simpson <cs@cskk.id.au> 25apr2018
#

from __future__ import print_function
from collections import defaultdict, namedtuple
from datetime import datetime, timedelta
from getopt import getopt, GetoptError
import os
from os.path import basename, splitext, join as joinpath, exists as pathexists
import re
from subprocess import run
import sys
from cs.csvutils import csv_reader
from cs.env import envsub
from cs.logutils import setup_logging, error, warning
from cs.pfx import Pfx
from cs.psutils import groupargv
from cs.py.func import prop

ENV_BASEDIR = 'SP_LINK_BASEDIR'
ENV_BASEDIR_DEFAULT = '$HOME/var/sp-link'

USAGE = r'''Usage: %s [-D basedir] [-d sitedir] op [op-args...]
  -D basedir
    Base directory for RRD file data, containing per-site subdirectories.
    Default: $''' + ENV_BASEDIR + ''' or ''' + ENV_BASEDIR_DEFAULT + '''
  -d sitedir
    Directory for RRD file data.
    Default: {basedir}/{site}
  Ops:
    graph csvfiles...
        Graph the data from the RRD files corresponding to the specified CSVs.
    import csvfiles...
        Import data from the specified CSVs into RRD files.
    parse csvfiles...'''

def main(argv=None):
  ''' Main programme implementing the command line.
  '''
  if argv is None:
    argv = sys.argv
  cmd = argv.pop(0)
  setup_logging(cmd)
  usage = USAGE % (cmd,)
  basedir = None
  sitedir = None
  badopts = False
  try:
    opts, argv = getopt(argv, 'D:d:')
  except GetoptError as e:
    error("%s", e)
    badopts = True
  else:
    for opt, value in opts:
      if opt == '-D':
        basedir = value
      elif opt == '-d':
        sitedir = value
      else:
        raise RuntimeError("unimplemented option: %s" % (opt,))
  cfg = SPConfig(basedir=basedir, sitedir=sitedir)
  try:
    if not argv:
      raise GetoptError("missing op")
    op = argv.pop(0)
    with Pfx(op):
      if op == 'graph':
        return cmd_graph(argv, cfg)
      if op == 'import':
        return cmd_import(argv, cfg)
      if op == 'parse':
        return cmd_parse(argv, cfg)
      raise GetoptError("unrecognised op")
  except GetoptError as e:
    error("%s", e)
    badopts = True
  if badopts:
    print(usage, file=sys.stderr)
    return 2
  return 0

UNIX_EPOCH = datetime(1970, 1, 1)
SP_EPOCH = datetime(2001, 1, 1).astimezone()

GRAPH_DAYS = 6 * 24 * 3600

RRD_VALID_DS_NAME = re.compile('^[a-zA-Z0-9_]{1,19}$')

# SP polls default to every 15 minutes, set heartbeat to 20 minutes in case of skew
##RRD_HEARTBEAT = 1200
RRD_HEARTBEAT = 1200

RRD_STEP = 10           # 10s slot size
RRD_SLOTS = 3153600     # a year in 10s slots

# scale factor for per-slot rates to get per-hour rates
# base rate is kwh/s
RRD_Y_SCALE = 3600

##RRD_STEP = 900
##RRD_SLOTS = 35040

KEY_TIMESTAMP_SECONDS = 'Date/Time Stamp [Seconds From The Year 2001]'

MAPPINGS = {
    'DetailedData': {
        'csv2rrd': {
            'Date/Time Stamp [Seconds From The Year 2001]': None,
            'Date/Time Stamp [dd/MM/yyyy - HH:mm:ss]': None,
            'Inverter AC Power (Average) [kW]': 'inverterACPwrKW:GAUGE',
            'DC Input Accumulated (Sample) [kWh]': 'dcInputCumKWH:DCOUNTER',
            'DC Output Accumulated (Sample) [kWh]': 'dcOutputCumKWH:DCOUNTER',
            'Battery In Accumulated (Sample) [kWh]': 'batteryInCumKWH:DCOUNTER',
            'Battery Out Accumulated (Sample) [kWh]': 'batteryOutCumKWH:DCOUNTER',
            'DC Voltage (Average) [V DC]': 'dcVoltsAvgVDC:GAUGE',
            'DC Voltage (Min) [V DC]': 'dcVoltsMinVDC:GAUGE',
            'DC Voltage (Max) [V DC]': 'dcVoltsMaxVDC:GAUGE',
            'DC Mid Voltage (Average) [V DC]': 'dcMidVoltsAvgVDC:GAUGE',
            'DC Mid Voltage (Sync Sample at Min DC V) [V DC]': 'dcMidVoltsMinVDC:GAUGE',
            'DC Mid Voltage (Sync Sample at Max DC V) [V DC]': 'dcMidVoltsMaxVDC:GAUGE',
            'Inverter DC Current (Average) [A]': 'invDCCurrentAvgA:GAUGE',
            'Shunt1 Current (Average) [A]': 'shunt1CurrentA:GAUGE',
            'Shunt 2 Current (Average) [A]': 'shunt2CurrentA:GAUGE',
            'Load AC Power (Average) [kW]': 'acLoadPwrAvgKW:GAUGE',
            'Load AC Power (Max) [kW]': 'acLoadPwrMaxKW:GAUGE',
            'AC Input Power (Average) [kW]': 'ACInputPwrAvgKW:GAUGE',
            'AC Load Voltage (Average) [V AC]': 'acLoadVoltsAvgVDC:GAUGE',
            'AC Load Frequency (Average) [Hz]': 'acLoadFreqAvgHz:GAUGE',
            'Transformer Temperature (Max) [Degrees C]': 'transformerTempMaxC:GAUGE',
            'Heatsink Temperature (Max) [Degrees C]': 'heatsinkTempMaxC:GAUGE',
            'Battery Temperature (Max) [Degrees C]': 'batteryTempMaxC:GAUGE',
            'Internal Temperature (Max) [Degrees C]': 'internalTempMaxC:GAUGE',
            'Power Module Temperature (Max) [Degrees C]': 'powerModTempMaxC:GAUGE',
            'State of Charge (Sample) [%]': 'batterySOCPcnt:GAUGE',
            'AC Input kWh Accumulated (Sample) [kWh]': 'acInputCumKWH:DCOUNTER',
            'AC Load kWh Accumulated (Sample) [kWh]': 'acLoadCumKWH:DCOUNTER',
            'Shunt 1 kWh Accumulated (Sample) [kWh]': 'shunt1SampCumKWH:DCOUNTER',
            'Shunt 2 kWh Accumulated (Sample) [kWh]': 'shunt2SampCumKWH:DCOUNTER',
            'Analogue In 1 DC Voltage (Average) [V DC]': 'alogIn1VoltsAvgVDC:GAUGE',
            'Analogue In 2 DC Voltage (Average) [V DC]': 'alogIn2VoltsAvgVDC:GAUGE',
            'AC Export kWh Accumulated (Sample) [kWh]': 'acExportCumKWH:DCOUNTER',
            'Total AC Coupled Power (Average) [kW]': 'totACCoupledPwrKW:GAUGE',
            'Total AC Coupled Energy (Sample) [kWh]': 'totACCoupledEgyKWH:DCOUNTER',
        },
        'graphs': {
            'battery_charge': [ 'DEF:batterySOCPcnt:batterySOCPcnt:MAX', 'LINE:batterySOCPcnt#000080:Battery SOC %' ],
            'power': [
                'DEF:ACLoadCumKWH:ACLoadCumKWH:LAST',
                f'CDEF:acloadkw=ACLoadCumKWH,{RRD_Y_SCALE},*',
                'DEF:pvinputkwh:dcInputCumKWH:LAST',
                f'CDEF:pvkw=pvinputkwh,{RRD_Y_SCALE},*,0,6,LIMIT',
                'DEF:BatInputCumKW:batteryInCumKWH:LAST',
                f'CDEF:BatChargeKW=BatInputCumKW,{RRD_Y_SCALE},*',
                'DEF:BatOutputCumKW:batteryOutCumKWH:LAST',
                f'CDEF:BatDischargeKW=BatOutputCumKW,{RRD_Y_SCALE},*',
                'DEF:ACExportCumKWH:ACExportCumKWH:LAST',
                f'CDEF:GridExpKW=ACExportCumKWH,{RRD_Y_SCALE},*',
                # the load should be met by the sum of:
                # pvload=min(load, pvinput)
                # batout=battery out
                # acimport=load-pvload-batout
                'CDEF:acloadafterbat=acloadkw,BatDischargeKW,-,0,MAX',
                'CDEF:acloadfrombat=acloadkw,acloadafterbat,-',
                'CDEF:batexport=BatDischargeKW,acloadfrombat,-',
                'CDEF:acloadafterpv=acloadafterbat,pvkw,-,0,MAX',
                'CDEF:acloadfrompv=acloadafterbat,acloadafterpv,-,0,MAX',
                'CDEF:acloadfromgrid=acloadafterpv',
                'CDEF:pvloadkw=acloadkw,pvkw,MIN',
                'CDEF:GridImportKW=acloadkw,pvloadkw,-,BatDischargeKW,-',
                'AREA:acloadfrompv#0000ff:Load From PV KW',
                'AREA:acloadfrombat#ff8080:Load From PV KW:STACK',
                'AREA:acloadfromgrid#ff0000:Grid Import KW (load - pv - battery):STACK',
                'AREA:BatChargeKW#8080ff:Battery Charge from PV? KW (from batteryInCumKWH):STACK',
                'AREA:batexport#ffff00:Battery Export? Battery not consumed by load:STACK',
                'AREA:GridExpKW#00ff00:Grid Export KW (from ACExportCumKWH):STACK',
                'LINE1:pvkw#c0c080:PV Input KW (from dcInputCumKWH)',
                'LINE1:acloadkw#000000:Load AC Power KW',
            ],
        },
    },
}

class SPConfig(namedtuple('SPConfig', 'basedir sitedir')):
  ''' An embodiment of the sp-link command's options.
  '''

  @prop
  def basepath(self):
    ''' Base directory for RRD file data, containing per-site subdirectories.
        Default: $''' + ENV_BASEDIR + ''' or ''' + ENV_BASEDIR_DEFAULT + '''
    '''
    base = self.basedir
    if base is None:
      base = os.environ.get(ENV_BASEDIR)
      if base is None:
        base = envsub(ENV_BASEDIR_DEFAULT)
    return base

  def sitepath(self, site):
    ''' Directory to hold the per-site files such as RRD files.
    '''
    sitedir = self.sitedir
    if sitedir is None:
      sitedir = joinpath(self.basepath, site)
    return sitedir

  @staticmethod
  def parse_csvfilename(filename):
    ''' Extract site and dataset name from a CSV filename. Return (site, datasetname, dumptime).
    '''
    filebase = basename(filename)
    root, ext = splitext(filebase)
    if ext.lower() != '.csv':
      raise ValueError("not a CSV file: %r" % (filename,))
    found = False
    for mkey in MAPPINGS.keys():
      _mkey_ = '_' + mkey + '_'
      try:
        site, dumptime = root.split(_mkey_, 1)
      except ValueError:
        continue
      found = True
      break
    if not found:
      raise ValueError("unrecognised CSV base: %r" % (filebase,))
    return site, mkey, dumptime

def cmd_graph(argv, cfg):
  ''' Run the "graph" command, generating graphs of RRD files associated with CSV data.
  '''
  xit = 0
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      dataset.rrd_graph()
  return xit

def cmd_import(argv, cfg):
  ''' Run the "import" command, loading CSV data into RRD files.
  '''
  xit = 0
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      rrdfile = dataset.rrdfile
      if not pathexists(rrdfile):
        dataset.rrd_create()
      dataset.rrd_update()
  return xit

def cmd_parse(argv, cfg):
  ''' Run the "parse" command, reading CSV data files.
  '''
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      data = dataset.data
      hdrs = dataset.data_hdrs
      print("%s: %d rows of %d columns" % (csvfile, len(data), len(hdrs)))
      for ndx, hdr in enumerate(hdrs):
        print(' ', ndx, hdr)
      print(list(dataset.rrd_field_defns()))

def load_csv(csvfp, no_headers=False):
  ''' Load data from a CSV file, return header names and data rows.
      If `no_headers` is true (default False) then the header names
        will be None and the rows will be lists. Otherwise the header
        names will be a list of strings and the rows will be dicts.
  '''
  if isinstance(csvfp, str):
    csvfile = csvfp
    with Pfx(csvfile):
      with open(csvfile) as csvfp:
        return load_csv(csvfp, no_headers=no_headers)
  r = csv_reader(csvfp)
  if no_headers:
    hdrs = None
    ts_index = None
  else:
    with Pfx("0"):
      hdrs = next(r)
    try:
      ts_index = hdrs.index(KEY_TIMESTAMP_SECONDS)
    except ValueError:
      ts_index = None
  nonzero = defaultdict(bool)
  data = []
  for rownum, row in enumerate(r, 1):
    with Pfx("%d: %s", rownum, row[1]):
      for i, v in enumerate(row):
        hdr = hdrs[i-1]
        with Pfx(hdr):
          try:
            v = int(v)
          except ValueError:
            try:
              v = float(v)
            except ValueError:
              v = v.strip()
              if hdr == 'Date/Time Stamp [Seconds From The Year 2001]':
                pass
              elif v == '':
                v = 0.0
              else:
                warning("string value: %r", v)
            else:
              if v != 0.0:
                nonzero[hdrs[i]] = True
          else:
            if v != 0:
              nonzero[hdrs[i]] = True
          row[i] = v
      if hdrs:
        row = dict(zip(hdrs, row))
        if ts_index is not None:
          row['unixtime'] = sp_unixtime(row[KEY_TIMESTAMP_SECONDS])
      data.append(row)
  for hdr, nz in sorted(nonzero.items()):
    if not nz:
      warning("%r: all zero or string", hdr)
  return hdrs, data

def sp_unixtime(sp_ts):
  ''' Take an SP LINK timestamp and produce a UNIX timestamp.
  '''
  dt = SP_EPOCH + timedelta(seconds=sp_ts)
  return dt.timestamp()

class DataSet(object):
  ''' Data and methods for a CSV file.
  '''

  def __init__(self, site, mapping_name, cfg):
    X("DataSet(mapping_name=%r, cfg=%r)...", mapping_name, cfg)
    self.site = site
    self.mapping_name = mapping_name
    self.cfg = cfg
    self.data = []
    self._mapping = MAPPINGS[self.mapping_name]
    self.csv2rrd = self._mapping['csv2rrd']
    self.data_hdrs = self.csv2rrd.keys()
    self.graphs = self._mapping['graphs']

  @classmethod
  def from_csvfile(cls, csvfile, cfg):
    ''' Construct a DataSet from a CSV file, and load the CSV data. Return the DataSet.
    '''
    csvfile_site, mapping_name, dumptime = cfg.parse_csvfilename(csvfile)
    DS = cls(csvfile_site, mapping_name, cfg)
    DS.import_csv(csvfile)
    return DS

  def import_csv(self, csvfile):
    ''' Read the CSV data file, sotre headers and time-sorted data.
    '''
    with Pfx(csvfile):
      csvfile_site, mapping_name, dumptime = self.cfg.parse_csvfilename(csvfile)
      if mapping_name != self.mapping_name:
        raise ValueError(
            "CSV mapping name %r does not match %s mapping name %r"
            % (mapping_name, type(self).__name__, self.mapping_name))
      hdrs, data = load_csv(csvfile)
      self.data.extend(data)
      self.data.sort(key=lambda row: row['unixtime'])

  def pathto(self, filebase):
    ''' Return the pathname of a file given its basename.
    '''
    return joinpath(self.cfg.sitepath(self.site), filebase)

  @prop
  def rrdfile(self):
    ''' Pathname for the RRD file containing this dataset.
    '''
    return self.pathto(f'{self.site}_{self.mapping_name}.rrd')

  @staticmethod
  def run_argv(pre_argv, argv):
    ''' Run a command which may be too long for the OS argument limit by breaking it into several commands if necessary.
    '''
    if argv:
      for argv_group in groupargv(pre_argv, argv):
        print(*argv_group, flush=True)
        run(argv_group, check=True)
    else:
      print(*pre_argv, flush=True)
      run(pre_argv, check=True)

  def rrd_specs(self):
    ''' Yield the broken out parts of the RRD field specifications.
    '''
    for row_key, rrd_spec in sorted(self.csv2rrd.items()):
      with Pfx("%r: %r", row_key, rrd_spec):
        if rrd_spec is None:
          continue
        rrd_field, rrd_type = rrd_spec.split(':')
        if not RRD_VALID_DS_NAME.match(rrd_field):
          raise ValueError("invalid ds-name: %r" % (rrd_field,))
        if rrd_type not in ('GAUGE', 'DCOUNTER'):
          raise ValueError("invalid ds-name type: %r" % (rrd_type,))
      yield row_key, rrd_field, rrd_type

  def rrd_field_defns(self):
    ''' Yield the RRD filed definitions for this DataSet.
    '''
    for _, rrd_field, rrd_type in self.rrd_specs():
      yield f"DS:{rrd_field}:{rrd_type}:{RRD_HEARTBEAT}:0:U"
    yield f"RRA:MAX:0.01:1:{RRD_SLOTS}"

  def rrd_create(self):
    ''' Create the RRD file for this DataSet.
    '''
    start_time = self.data[0]['unixtime']
    self.run_argv([
        'rrdtool', 'create', self.rrdfile,
        '--step', str(RRD_STEP),
        '--start', str(int(start_time-RRD_STEP*(RRD_SLOTS+1))),
    ], list(self.rrd_field_defns()))

  def rrd_update(self):
    ''' Update the RRD file with values from this DataSet.
    '''
    pre_argv = [
        'rrdtool', 'update', self.rrdfile,
        '--template', ':'.join(rrd_field for row_key, rrd_field, rrd_type in self.rrd_specs()),
        '--skip-past-updates',
        '--'
    ]
    data_argv = []
    row_time_prev = None
    for row in self.data:
      row_time = row['unixtime']
      if row_time_prev is not None and row_time <= row_time_prev:
        error("SKIP out of order rows: row_time=%s, row_time_prev=%s", row_time, row_time_prev)
        continue
      row_time_prev = row_time
      values = [row_time]
      for row_key, _, _ in self.rrd_specs():
        values.append(row[row_key])
      data_argv.append(':'.join(str(value) for value in values))
    self.run_argv(pre_argv, data_argv)

  def rrd_graph(self):
    ''' Generate the various graphs defined for this DataSet.
    '''
    ##start = int(self.data[0]['unixtime'])
    end = int(self.data[-1]['unixtime'])
    start = end - GRAPH_DAYS
    for graph_name, graph_spec in self.graphs.items():
      graphfile = self.pathto(f"{self.site}_{self.mapping_name}_{graph_name}.png")
      with Pfx(graphfile):
        rrd_argv = [
            'rrdtool', 'graph', graphfile,
            f'--start={start}', f'--end={end}',
            '--width=2048', '--height=512'
        ]
        for spec in graph_spec:
          with Pfx(spec):
            fields = spec.split(':')
            spec_type = fields[0]
            if spec_type == 'DEF':
              # append the RRD file to the vname
              fields[1] = f"{fields[1]}={self.rrdfile}"
            elif spec_type == 'LINE':
              fields[0] = fields[0] + '1'
            spec = ':'.join(fields)
            rrd_argv.append(spec)
        self.run_argv(rrd_argv, [])
    if False:
      # debugging graphs for various parameters
      for hdr in sorted(self.data_hdrs):
        rrdspec = self.csv2rrd[hdr]
        if rrdspec is None:
          continue
        rrdfield, _ = rrdspec.split(':')
        graphfile = f"{self.csvfile_site}_{self.mapping_name}_hdr_{rrdfield}.png"
        rrd_argv = [
            'rrdtool', 'graph', graphfile,
            f'--start={start}', f'--end={end}', '--width=2048',
            f"DEF:{rrdfield}={self.rrdfile}:{rrdfield}:MAX",
            f"LINE1:{rrdfield}#000000:Field {rrdfield} - {hdr}",
        ]
        self.run_argv(rrd_argv, [])

if __name__ == '__main__':
  sys.exit(main(sys.argv))
