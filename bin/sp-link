#!/usr/bin/env python3
#
# Utility script for dealing with SP-Link data.
# - Cameron Simpson <cs@cskk.id.au> 25apr2018
#

from __future__ import print_function
from collections import defaultdict, namedtuple
from datetime import datetime, timedelta
from getopt import getopt, GetoptError
import os
from os.path import basename, splitext, join as joinpath, exists as pathexists
import re
from subprocess import run
import sys
from cs.csvutils import csv_import, csv_reader
from cs.env import envsub
from cs.logutils import setup_logging, error, warning
from cs.pfx import Pfx
from cs.psutils import groupargv
from cs.py.func import prop
from cs.x import X

ENV_BASEDIR = 'SP_LINK_BASEDIR'
ENV_BASEDIR_DEFAULT = '$HOME/var/sp-link'

# TODO: needs to come from a config in the site dir
BATTERY_CAPACITY_KWH = 10.0

GRAPH_DAYS = 6 * 24 * 3600

RRD_VALID_DS_NAME = re.compile('^[a-zA-Z0-9_]{1,19}$')

# SP polls default to every 15 minutes, set heartbeat to 20 minutes in case of skew
RRD_HEARTBEAT = 1200

RRD_STEP = 10           # 10s slot size
RRD_SLOTS = 3153600     # a year in 10s slots

USAGE = r'''Usage: %s [-D basedir] [-d sitedir] op [op-args...]
  -D basedir
    Base directory for RRD file data, containing per-site subdirectories.
    Default: $''' + ENV_BASEDIR + ''' or ''' + ENV_BASEDIR_DEFAULT + '''
  -d sitedir
    Directory for RRD file data.
    Default: {basedir}/{site}
  Ops:
    graph [days] csvfiles...
        Graph the data from the RRD files corresponding to the specified CSVs.
    import csvfiles...
        Import data from the specified CSVs into RRD files.
    parse csvfiles...'''

def main(argv=None):
  ''' Main programme implementing the command line.
  '''
  if argv is None:
    argv = sys.argv
  cmd = argv.pop(0)
  setup_logging(cmd)
  usage = USAGE % (cmd,)
  basedir = None
  sitedir = None
  badopts = False
  try:
    opts, argv = getopt(argv, 'D:d:')
  except GetoptError as e:
    error("%s", e)
    badopts = True
  else:
    for opt, value in opts:
      if opt == '-D':
        basedir = value
      elif opt == '-d':
        sitedir = value
      else:
        raise RuntimeError("unimplemented option: %s" % (opt,))
  cfg = SPConfig(basedir=basedir, sitedir=sitedir)
  try:
    if not argv:
      raise GetoptError("missing op")
    op = argv.pop(0)
    with Pfx(op):
      if op == 'graph':
        return cmd_graph(argv, cfg)
      if op == 'import':
        return cmd_import(argv, cfg)
      if op == 'parse':
        return cmd_parse(argv, cfg)
      raise GetoptError("unrecognised op")
  except GetoptError as e:
    error("%s", e)
    badopts = True
  if badopts:
    print(usage, file=sys.stderr)
    return 2
  return 0

UNIX_EPOCH = datetime(1970, 1, 1)
SP_EPOCH = datetime(2001, 1, 1).astimezone()

# scale factor for per-slot rates to get per-hour rates
# base rate is kwh/s, this converts to kw
RRD_Y_SCALE = 3600

##RRD_STEP = 900
##RRD_SLOTS = 35040

KEY_TIMESTAMP_SECONDS = 'Date/Time Stamp [Seconds From The Year 2001]'

# RRD ds-name information:
# csv_hdr: the key from the CSV file, and index in the 'csv2rrd' map
# ds_name: the RRD ds-name
# ds_type: the RRD ds-name type
RRD_DSName = namedtuple('RRD_DSName', 'csv_hdr ds_name ds_type')

MAPPINGS = {
    'DetailedData': {
        'csv2rrd': {
            'Date/Time Stamp [Seconds From The Year 2001]': None,
            'Date/Time Stamp [dd/MM/yyyy - HH:mm:ss]': None,
            'Inverter AC Power (Average) [kW]': 'inverterACPwrKW:GAUGE',
            'DC Input Accumulated (Sample) [kWh]': 'dcInputCumKWH:DCOUNTER',
            'DC Output Accumulated (Sample) [kWh]': 'dcOutputCumKWH:DCOUNTER',
            'Battery In Accumulated (Sample) [kWh]': 'batteryInCumKWH:DCOUNTER',
            'Battery Out Accumulated (Sample) [kWh]': 'batteryOutCumKWH:DCOUNTER',
            'DC Voltage (Average) [V DC]': 'dcVoltsAvgVDC:GAUGE',
            'DC Voltage (Min) [V DC]': 'dcVoltsMinVDC:GAUGE',
            'DC Voltage (Max) [V DC]': 'dcVoltsMaxVDC:GAUGE',
            'DC Mid Voltage (Average) [V DC]': 'dcMidVoltsAvgVDC:GAUGE',
            'DC Mid Voltage (Sync Sample at Min DC V) [V DC]': 'dcMidVoltsMinVDC:GAUGE',
            'DC Mid Voltage (Sync Sample at Max DC V) [V DC]': 'dcMidVoltsMaxVDC:GAUGE',
            'Inverter DC Current (Average) [A]': 'invDCCurrentAvgA:GAUGE',
            'Shunt1 Current (Average) [A]': 'shunt1CurrentA:GAUGE',
            'Shunt 2 Current (Average) [A]': 'shunt2CurrentA:GAUGE',
            'Load AC Power (Average) [kW]': 'acLoadPwrAvgKW:GAUGE',
            'Load AC Power (Max) [kW]': 'acLoadPwrMaxKW:GAUGE',
            'AC Input Power (Average) [kW]': 'acInputPwrAvgKW:GAUGE',
            'AC Load Voltage (Average) [V AC]': 'acLoadVoltsAvgVDC:GAUGE',
            'AC Load Frequency (Average) [Hz]': 'acLoadFreqAvgHz:GAUGE',
            'Transformer Temperature (Max) [Degrees C]': 'transformerTempMaxC:GAUGE',
            'Heatsink Temperature (Max) [Degrees C]': 'heatsinkTempMaxC:GAUGE',
            'Battery Temperature (Max) [Degrees C]': 'batteryTempMaxC:GAUGE',
            'Internal Temperature (Max) [Degrees C]': 'internalTempMaxC:GAUGE',
            'Power Module Temperature (Max) [Degrees C]': 'powerModTempMaxC:GAUGE',
            'State of Charge (Sample) [%]': 'batterySOCPcnt:GAUGE',
            'AC Input kWh Accumulated (Sample) [kWh]': 'acInputCumKWH:DCOUNTER',
            'AC Load kWh Accumulated (Sample) [kWh]': 'acLoadCumKWH:DCOUNTER',
            'Shunt 1 kWh Accumulated (Sample) [kWh]': 'shunt1SampCumKWH:DCOUNTER',
            'Shunt 2 kWh Accumulated (Sample) [kWh]': 'shunt2SampCumKWH:DCOUNTER',
            'Analogue In 1 DC Voltage (Average) [V DC]': 'alogIn1VoltsAvgVDC:GAUGE',
            'Analogue In 2 DC Voltage (Average) [V DC]': 'alogIn2VoltsAvgVDC:GAUGE',
            'AC Export kWh Accumulated (Sample) [kWh]': 'acExportCumKWH:DCOUNTER',
            'Total AC Coupled Power (Average) [kW]': 'totACCoupledPwrKW:GAUGE',
            'Total AC Coupled Energy (Sample) [kWh]': 'totACCoupledEgyKWH:DCOUNTER',
        },
        'graphs': {
            'battery_charge': [
                'DEF:batterySOCPcnt:batterySOCPcnt:MAX',
                'LINE:batterySOCPcnt#000080:Battery SOC %'
            ],
            'ac': [
                'LINE1:acInputPwrAvgKW#ff0000:AC Input Power Avg KW',
                'LINE1:acInputCumKWH#ff8080:AC Input kWh Accumulated (Sample) [kWh]',
                'LINE1:acExportCumKWH#00ff00:AC Export kWh Accumulated (Sample) [kWh]',
                'LINE1:totACCoupledPwrKW#0000ff:Total AC Coupled Power (Average) [kW]',
                'LINE1:totACCoupledEgyKWH#00ff00:Total AC Coupled Energy (Sample) [kWh]',
                'LINE1:acLoadPwrAvgKW#00000:Load AC Power (Average) [kW]',
            ],
            'power': [
                f'CDEF:acloadkw=acLoadCumKWH,{RRD_Y_SCALE},*',
                f'CDEF:acloadkw_=acloadkw,300,TRENDNAN',
                f'CDEF:acloadmaxkw=acLoadCumKWH,{RRD_Y_SCALE},*',
                f'CDEF:pvkw=dcInputCumKWH,{RRD_Y_SCALE},*,0,6,LIMIT',
                f'CDEF:batChargeKW=batteryInCumKWH,{RRD_Y_SCALE},*',
                f'CDEF:batDischargeKW=batteryOutCumKWH,{RRD_Y_SCALE},*',
                f'CDEF:batDischargeKW_=batDischargeKW,300,TRENDNAN',
                f'CDEF:gridExpKW=acExportCumKWH,{RRD_Y_SCALE},*',
                # the load should be met by the sum of:
                # pvload=min(load, pvinput)
                # batout=battery out
                # acimport=load-pvload-batout
                'CDEF:acloadafterbat=acloadkw_,batDischargeKW_,-,0,MAX',
                'CDEF:acloadfrombat=acloadkw_,acloadafterbat,-',
                'CDEF:batExport=batDischargeKW_,acloadfrombat,-',
                'CDEF:acloadafterpv=acloadafterbat,pvkw,-,0,MAX',
                'CDEF:acloadfrompv=acloadafterbat,acloadafterpv,-,0,MAX',
                'CDEF:acloadfromgrid=acloadafterpv',
                'CDEF:pvloadkw=acloadkw_,pvkw,MIN',
                'CDEF:gridImportKW=acloadkw_,pvloadkw,-,batDischargeKW,-',
                'AREA:acloadfrompv#0000ff:Load From PV KW',
                'AREA:acloadfrombat#ff8080:Load From Battery KW:STACK',
                'AREA:acloadfromgrid#ff0000:Grid Import KW (load - pv - battery):STACK',
                'AREA:batChargeKW#8080ff:Battery Charge from PV? KW (from batteryInCumKWH):STACK',
                'AREA:batExport#ffff00:Battery Export? Battery not consumed by load:STACK',
                'AREA:gridExpKW#00ff00:Grid Export KW (from acExportCumKWH):STACK',
                'LINE1:acloadkw_#000000:Load AC Power KW Smoothed',
                ##'LINE1:acloadmaxkw#00ff00:Load AC Power KW',
                ##'LINE1:pvkw#c0c080:PV Input KW (from dcInputCumKWH)',
                'CDEF:batterySOC6KW=batterySOCPcnt,6,*,100,/',
                'LINE1:batterySOC6KW#404080:Battery SOC',
            ],
        },
    },
}

class SPConfig(namedtuple('SPConfig', 'basedir sitedir')):
  ''' An embodiment of the sp-link command's options.
  '''

  @prop
  def basepath(self):
    ''' Base directory for RRD file data, containing per-site subdirectories.
        Default: $''' + ENV_BASEDIR + ''' or ''' + ENV_BASEDIR_DEFAULT + '''
    '''
    base = self.basedir
    if base is None:
      base = os.environ.get(ENV_BASEDIR)
      if base is None:
        base = envsub(ENV_BASEDIR_DEFAULT)
    return base

  def sitepath(self, site):
    ''' Directory to hold the per-site files such as RRD files.
    '''
    sitedir = self.sitedir
    if sitedir is None:
      sitedir = joinpath(self.basepath, site)
    return sitedir

  @staticmethod
  def parse_csvfilename(filename):
    ''' Extract site and dataset name from a CSV filename. Return (site, datasetname, dumptime).
    '''
    filebase = basename(filename)
    root, ext = splitext(filebase)
    if ext.lower() != '.csv':
      raise ValueError("not a CSV file: %r" % (filename,))
    found = False
    for mkey in MAPPINGS.keys():
      _mkey_ = '_' + mkey + '_'
      try:
        site, dumptime = root.split(_mkey_, 1)
      except ValueError:
        continue
      found = True
      break
    if not found:
      raise ValueError("unrecognised CSV base: %r" % (filebase,))
    return site, mkey, dumptime

def cmd_graph(argv, cfg):
  ''' Run the "graph" command, generating graphs of RRD files associated with CSV data.
  '''
  xit = 0
  days = None
  if argv:
    try:
      days = int(argv[0])
    except ValueError:
      pass
    else:
      if days > 0:
        argv.pop(0)
      else:
        days = None
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      dataset.gen_graphs(days=days)
  return xit

def cmd_import(argv, cfg):
  ''' Run the "import" command, loading CSV data into RRD files.
  '''
  xit = 0
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      rrdfile = dataset.rrdfile
      if not pathexists(rrdfile):
        dataset.rrd_create()
      dataset.rrd_update()
  return xit

def cmd_parse(argv, cfg):
  ''' Run the "parse" command, reading CSV data files.
  '''
  if not argv:
    raise GetoptError("missing csvfiles")
  for csvfile in argv:
    with Pfx(csvfile):
      try:
        dataset = DataSet.from_csvfile(csvfile, cfg)
      except ValueError as e:
        warning("unhandled file: %s" % (e,))
        continue
      data = dataset.data
      hdrs = dataset.data_hdrs
      print("%s: %d rows of %d columns" % (csvfile, len(data), len(hdrs)))
      for ndx, hdr in enumerate(hdrs):
        print(' ', ndx, hdr)
      print(list(dataset.rrd_field_defns()))

def sp_unixtime(sp_ts):
  ''' Take an SP LINK timestamp and produce a UNIX timestamp.
  '''
  dt = SP_EPOCH + timedelta(seconds=sp_ts)
  return dt.timestamp()

class DataSet(object):
  ''' Data and methods for a CSV file.
  '''

  def __init__(self, site, mapping_name, cfg):
    X("DataSet(mapping_name=%r, cfg=%r)...", mapping_name, cfg)
    self.site = site
    self.mapping_name = mapping_name
    self.cfg = cfg
    self.data = []
    self._mapping = MAPPINGS[self.mapping_name]
    self.csv2rrd = self._mapping['csv2rrd']
    self.data_hdrs = self.csv2rrd.keys()
    self.graphs = self._mapping['graphs']

  @classmethod
  def from_csvfile(cls, csvfile, cfg):
    ''' Construct a DataSet from a CSV file, and load the CSV data. Return the DataSet.
    '''
    csvfile_site, mapping_name, dumptime = cfg.parse_csvfilename(csvfile)
    DS = cls(csvfile_site, mapping_name, cfg)
    DS.import_csv(csvfile)
    return DS

  def import_csv(self, csvfile):
    ''' Read the CSV data file, sotre headers and time-sorted data.
    '''
    with Pfx(csvfile):
      csvfile_site, mapping_name, dumptime = self.cfg.parse_csvfilename(csvfile)
      if mapping_name != self.mapping_name:
        raise ValueError(
            "CSV mapping name %r does not match %s mapping name %r"
            % (mapping_name, type(self).__name__, self.mapping_name))
      def preprocess(context, row):
        ''' Convert the CSV values, which are all strings, to numeric values.
        '''
        if context.index > 0:
          for i, v in enumerate(row):
            try:
              v = int(v)
            except ValueError:
              try:
                v = float(v)
              except ValueError:
                v = v.strip()
                if v == '':
                  v = 0.0
            row[i] = v
        return row
      with open(csvfile) as csvfp:
        row_class, row_data = csv_import(
            csvfp,
            computed={
                'unixtime':
                    lambda self: sp_unixtime(self[KEY_TIMESTAMP_SECONDS])
            },
            preprocess=preprocess)
        X("row_class=%s", row_class)
        self.data.extend(row_data)
    self.data.sort(key=lambda row: row['unixtime'])

  def pathto(self, filebase):
    ''' Return the pathname of a file given its basename.
    '''
    return joinpath(self.cfg.sitepath(self.site), filebase)

  @prop
  def rrdfile(self):
    ''' Pathname for the RRD file containing this dataset.
    '''
    return self.pathto(f'{self.site}_{self.mapping_name}.rrd')

  @staticmethod
  def run_argv(pre_argv, argv):
    ''' Run a command which may be too long for the OS argument limit by breaking it into several commands if necessary.
    '''
    if argv:
      for argv_group in groupargv(pre_argv, argv):
        print(*argv_group, flush=True)
        run(argv_group, check=True)
    else:
      print(*pre_argv, flush=True)
      run(pre_argv, check=True)

  def rrd_specs(self):
    ''' Yield the broken out parts of the RRD field specifications.
    '''
    for row_key, rrd_spec in sorted(self.csv2rrd.items()):
      with Pfx("%r: %r", row_key, rrd_spec):
        if rrd_spec is None:
          continue
        rrd_field, rrd_type = rrd_spec.split(':')
        if not RRD_VALID_DS_NAME.match(rrd_field):
          raise ValueError("invalid ds-name: %r" % (rrd_field,))
        if rrd_type not in ('GAUGE', 'DCOUNTER'):
          raise ValueError("invalid ds-name type: %r" % (rrd_type,))
      yield RRD_DSName(row_key, rrd_field, rrd_type)

  def rrd_field_defns(self):
    ''' Yield the RRD filed definitions for this DataSet.
    '''
    for rrd_spec in self.rrd_specs():
      yield f"DS:{rrd_spec.ds_name}:{rrd_spec.ds_type}:{RRD_HEARTBEAT}:0:U"
    yield f"RRA:MAX:0.01:1:{RRD_SLOTS}"

  def rrd_create(self):
    ''' Create the RRD file for this DataSet.
    '''
    start_time = self.data[0]['unixtime']
    self.run_argv([
        'rrdtool', 'create', self.rrdfile,
        '--step', str(RRD_STEP),
        '--start', str(int(start_time-RRD_STEP*(RRD_SLOTS+1))),
    ], list(self.rrd_field_defns()))

  def rrd_update(self):
    ''' Update the RRD file with values from this DataSet.
    '''
    pre_argv = [
        'rrdtool', 'update', self.rrdfile,
        '--template', ':'.join(rrd_spec.ds_name for rrd_spec in self.rrd_specs()),
        '--skip-past-updates',
        '--'
    ]
    data_argv = []
    row_time_prev = None
    for row in self.data:
      row_time = row['unixtime']
      if row_time_prev is not None and row_time <= row_time_prev:
        error("SKIP out of order rows: row_time=%s, row_time_prev=%s", row_time, row_time_prev)
        continue
      row_time_prev = row_time
      values = [row_time]
      for rrd_spec in self.rrd_specs():
        values.append(row[rrd_spec.csv_hdr])
      data_argv.append(':'.join(str(value) for value in values))
    self.run_argv(pre_argv, data_argv)

  def rrdgraph_ds_defs(self):
    ''' Yield DEFs for each ds-name in the RRD file, used to prime rrdgraph incantations.
    '''
    for rrd_spec in self.rrd_specs():
      yield f'DEF:{rrd_spec.ds_name}={self.rrdfile}:{rrd_spec.ds_name}:MAX'

  def rrdgraph(self, graph_name, graph_argv,
      start='end-1000000', end=None,
      width=2048, height=512):
    graphfile = self.pathto(f"{self.site}_{self.mapping_name}_{graph_name}.png")
    with Pfx(graphfile):
      rrd_argv = [
          'rrdtool', 'graph', graphfile,
          f'--width={width}', f'--height={height}'
      ]
      if start is not None:
        rrd_argv.append(f'--start={start}')
      if end is not None:
        rrd_argv.append(f'--end={end}')
      rrd_argv.extend(self.rrdgraph_ds_defs())
      rrd_argv.extend(graph_argv)
      self.run_argv(rrd_argv, [])

  def gen_graphs(self, days=None):
    ''' Generate the various graphs defined for this DataSet.
    '''
    if days is None:
      days = GRAPH_DAYS
    time_period = days * 24 * 3600
    ##start = int(self.data[0]['unixtime'])
    ##end = int(self.data[-1]['unixtime'])
    end_spec = 'now'
    start_spec = 'end-%ds' % (time_period,)
    for graph_name, graph_spec in self.graphs.items():
      graph_argv = []
      for spec in graph_spec:
        with Pfx(spec):
          fields = spec.split(':')
          spec_type = fields[0]
          if spec_type == 'DEF':
            # append the RRD file to the vname
            fields[1] = f"{fields[1]}={self.rrdfile}"
          elif spec_type == 'LINE':
            fields[0] = fields[0] + '1'
          spec = ':'.join(fields)
          graph_argv.append(spec)
      self.rrdgraph(graph_name, graph_argv, start=start_spec, end=end_spec)
    graph_argv = []
    for rrd_spec in self.rrd_specs():
      if 'kwh' in rrd_spec.csv_hdr.lower():
        graph_argv.append(f'LINE1:{rrd_spec.ds_name}:{rrd_spec.csv_hdr} {rrd_spec.ds_name}')
    self.rrdgraph('kwh', graph_argv, start=start_spec, end=end_spec)
    graph_argv = []
    for rrd_spec in self.rrd_specs():
      if 'kw' in rrd_spec.csv_hdr.lower() and 'kwh' not in rrd_spec.csv_hdr.lower():
        graph_argv.append(f'LINE1:{rrd_spec.ds_name}:{rrd_spec.csv_hdr} {rrd_spec.ds_name}')
    self.rrdgraph('kw', graph_argv, start=start_spec, end=end_spec)
    if True:
      # debugging graphs for various parameters
      for rrd_spec in self.rrd_specs():
        self.rrdgraph(f'ds_{rrd_spec.ds_name}', [
            f"DEF:{rrd_spec.ds_name}={self.rrdfile}:{rrd_spec.ds_name}:MAX",
            f"LINE1:{rrd_spec.ds_name}#000000:Field {rrd_spec.ds_name} - {rrd_spec.csv_hdr}",
        ], start=start_spec, end=end_spec)

if __name__ == '__main__':
  sys.exit(main(sys.argv))
